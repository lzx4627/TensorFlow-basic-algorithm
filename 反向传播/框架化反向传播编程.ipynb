{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "框架化反向传播编程\n",
    "@author: 知乎@Ai酱\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "class Layer(object):\n",
    "    '''\n",
    "    本文中，一层只有一个神经元，一个神经元只有一个输入一个输出\n",
    "    '''\n",
    "    def __init__(self,layer_index):\n",
    "        '''\n",
    "        layer_index: 第几层\n",
    "        '''\n",
    "        self.layer_index = layer_index\n",
    "        # 初始化权重[0,1] - 0.5 = [-0.5,0.5]保证初始化有正有负\n",
    "        self.w = random.random() - 0.5 \n",
    "        # 当前层的输出\n",
    "        self.output = 0\n",
    "        \n",
    "    def forward(self,input_var):\n",
    "        '''\n",
    "        前向传播：对输入进行运算，并将结果保存\n",
    "        input_var: 当前层的输入\n",
    "        '''\n",
    "        self.input = input_var\n",
    "        self.output = self.w * self.input\n",
    "        \n",
    "    \n",
    "    def backward(self, public_value):\n",
    "        '''\n",
    "        反向传播：计算上层也会使用的导数值并保存\n",
    "        假设当前层的计算规则是这样output = f(input)，\n",
    "        而 input == 前一层的输出，\n",
    "        因此，根据链式法则损失函数对上一层权重的导数 = 后面层传过来的公共导数* f'(input) * 前一层的导数\n",
    "        也就是说，后面层传过来的公共导数值* f'(input) 是需要往前传的公用的导数值。\n",
    "        由于本层中对输入做的运算为：output = f(input) = w*input\n",
    "        所以, f'(input) = w.\n",
    "        public_value: 后面传过来的公共导数值\n",
    "        '''\n",
    "        # 当前层要传给前面层的公共导数值 = 后面传过来的公共导数值 * f'(input)\n",
    "        self.public_value = public_value * self.w\n",
    "        # 损失函数对当前层参数w的导数 = 后面传过来的公共导数值 * f'(input) * doutput/dw\n",
    "        self.w_grad = self.public_value * self.input\n",
    "    \n",
    "    def upate(self, learning_rate):\n",
    "        '''\n",
    "        利用梯度下降更新参数w\n",
    "        参数迭代更新规则（梯度下降）： w = w - 学习率*损失函数对w的导数\n",
    "        learning_rate: 学习率\n",
    "        '''\n",
    "        self.w = self.w - learning_rate * self.w_grad\n",
    "    \n",
    "    def display(self):\n",
    "        print('layer',self.layer_index,'w:',self.w)\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self,layers_num):\n",
    "        '''\n",
    "        构造网络\n",
    "        layers_num: 网络层数\n",
    "        '''\n",
    "        self.layers = []\n",
    "        # 向网络添加层\n",
    "        for i in range(layers_num):\n",
    "            self.layers.append(Layer(i+1))#层编号从1开始\n",
    "    \n",
    "    def predict(self, sample):\n",
    "        '''\n",
    "        sample: 样本输入\n",
    "        return 最后一层的输出\n",
    "        '''\n",
    "        output = sample\n",
    "        for layer in self.layers:\n",
    "            layer.forward(output)\n",
    "            output = layer.output\n",
    "        return 1 if output>0 else -1\n",
    "    \n",
    "    def calc_gradient(self, label):\n",
    "        '''\n",
    "        从后往前计算损失函数对各层的导数\n",
    "        '''\n",
    "        # 计算最后一层的导数\n",
    "        last_layer = self.layers[-1]\n",
    "        # 由于损失函数=0.5*(last_layer.output - label)^2\n",
    "        # 由于backward中的public_value = 输出对输入的导数\n",
    "        # 对于损失函数其输入是last_layer.output，损失函数对该输入的导数=last_layer.output - label\n",
    "        # 所以 最后一层的backward的public_value = last_layer.output - label\n",
    "        last_layer.backward(last_layer.output - label)\n",
    "        public_value = last_layer.public_value\n",
    "        for layer in self.layers:\n",
    "            layer.backward(public_value) # 计算损失函数对该层参数的导数\n",
    "            public_value= layer.public_value\n",
    "            \n",
    "    def update_weights(self, learning_rate):\n",
    "        '''\n",
    "        更新各层权重\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            layer.upate(learning_rate)\n",
    "        \n",
    "    \n",
    "    def train_one_sample(self, label, sample, learning_rate):\n",
    "        self.predict(sample) # 前向传播，使得各层的输入都有值\n",
    "        self.calc_gradient(label) # 计算各层导数\n",
    "        self.update_weights(learning_rate) # 更新各层参数\n",
    "        \n",
    "    def train(self, labels, data_set, learning_rate, epoch):\n",
    "        '''\n",
    "        训练神经网络\n",
    "        labels: 样本标签\n",
    "        data_set: 输入样本们\n",
    "        learning_rate: 学习率\n",
    "        epoch: 同样的样本反复训练的次数\n",
    "        '''\n",
    "        for _ in range(epoch):# 同样数据反复训练epoch次保证权重收敛\n",
    "            for i in range(len(labels)):#逐样本更新权重\n",
    "                self.train_one_sample(labels[i], data_set[i], learning_rate)\n",
    "\n",
    "nn = Network(3)\n",
    "data_set = [1,-1]\n",
    "labels   = [-1,1]\n",
    "learning_rate = 0.05\n",
    "epoch = 160\n",
    "nn.train(labels,data_set,learning_rate,epoch)\n",
    "print(nn.predict(1)) # 输出 -1\n",
    "print(nn.predict(-1)) # 输出 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
